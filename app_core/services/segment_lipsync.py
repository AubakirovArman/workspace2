"""Segmented lipsync generation helpers."""
from __future__ import annotations

import json
import numbers
import subprocess
import threading
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torchaudio
import wave

from .. import state
from ..config import AVATAR_FPS, AVATAR_IMAGE, OUTPUT_DIR, TEMP_DIR
from .tts import convert_to_wav, generate_tts

FACE_PADS = (0, 50, 0, 0)
VIDEO_EXTENSIONS = {".mp4", ".mov", ".avi", ".mkv", ".webm", ".m4v"}
SEGMENT_META_FILENAME = "meta.json"


def _ensure_service_cuda_device(service) -> None:
    if not torch.cuda.is_available():
        return
    target_device = getattr(service, "device", None)
    if target_device is None:
        return
    try:
        torch.cuda.set_device(torch.device(str(target_device)))
    except Exception:
        pass


@dataclass
class SegmentEncodingResult:
    """Metadata about a single encoded video segment."""

    index: int
    frame_start: int
    frame_end: int
    frame_count: int
    write_time: float
    ffmpeg_time: float
    return_code: int
    stderr: str = ""

    def to_dict(self) -> Dict[str, object]:
        return {
            "index": self.index,
            "frame_start": self.frame_start,
            "frame_end": self.frame_end,
            "frame_count": self.frame_count,
            "write_time": self.write_time,
            "ffmpeg_time": self.ffmpeg_time,
            "return_code": self.return_code,
            "stderr": self.stderr,
        }


@dataclass
class SegmentTiming:
    """Timing breakdown for segmented generation."""

    tts_time: float
    audio_convert_time: float
    capture_time: float
    inference_time: float
    encode_time: float
    merge_time: float
    audio_mux_time: float
    total_time: float

    def to_dict(self) -> Dict[str, float]:
        return asdict(self)


@dataclass
class SegmentJobResult:
    """Final result of segmented lip-sync generation."""

    job_id: str
    video_path: Path
    temp_dir: Path
    segments: int
    requested_segments: Optional[int]
    total_frames: int
    resolution: Tuple[int, int]
    timings: SegmentTiming
    segment_results: List[SegmentEncodingResult]
    inference_stats: Dict[str, float]
    capture_workers: int
    capture_chunks: int

    def dump_metadata(self) -> None:
        payload = {
            "job_id": self.job_id,
            "video_path": str(self.video_path),
            "segments": self.segments,
            "requested_segments": self.requested_segments,
            "total_frames": self.total_frames,
            "resolution": {
                "height": int(self.resolution[0]),
                "width": int(self.resolution[1]),
            },
            "timings": self.timings.to_dict(),
            "segment_results": [segment.to_dict() for segment in self.segment_results],
            "inference_stats": self.inference_stats,
            "capture_workers": self.capture_workers,
            "capture_chunks": self.capture_chunks,
            "created_at": datetime.utcnow().isoformat() + "Z",
        }
        meta_path = self.temp_dir / SEGMENT_META_FILENAME
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        with open(meta_path, "w", encoding="utf-8") as handle:
            json.dump(payload, handle, ensure_ascii=False, indent=2)


def run_segmented_lipsync(
    text: str,
    language: str = "ru",
    segments: int = 16,
    batch_size: int = 1024,
) -> SegmentJobResult:
    text = text.strip()
    if not text:
        raise ValueError("Ð¢ÐµÐºÑÑ‚ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ð¼")

    gan_services = [svc for svc in state.get_all_gan_services() if svc]
    service_pool = list(gan_services)
    if not service_pool and state.lipsync_service_nogan is not None:
        service_pool = [state.lipsync_service_nogan]

    if not service_pool:
        raise RuntimeError("ÐœÐ¾Ð´ÐµÐ»ÑŒ lipsync Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°")

    primary_service = service_pool[0]

    if len(service_pool) > 1:
        device_labels = ", ".join(str(getattr(svc, "device", "cuda")) for svc in service_pool)
        print(f"ðŸ§  Ð¡ÐµÐ³Ð¼ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ {len(service_pool)} GAN ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²: {device_labels}")

    static_mode = bool(state.avatar_static_mode or not _avatar_supports_video())

    start_total = time.perf_counter()

    start_tts = time.perf_counter()
    audio_data = generate_tts(text, language)
    tts_time = time.perf_counter() - start_tts

    start_convert = time.perf_counter()
    audio_waveform, audio_sample_rate = convert_to_wav(audio_data, None)
    audio_convert_time = time.perf_counter() - start_convert

    audio_samples = int(audio_waveform.shape[1])
    audio_duration_seconds = audio_samples / float(audio_sample_rate)

    job_id = datetime.now().strftime("segment_%Y%m%d_%H%M%S_%f")
    temp_root = Path(TEMP_DIR) / "segments" / job_id
    segments_dir = temp_root / "chunks"
    segments_dir.mkdir(parents=True, exist_ok=True)

    audio_path = temp_root / "audio.wav"
    _save_waveform(audio_path, audio_waveform, audio_sample_rate)

    for index, svc in enumerate(service_pool, start=1):
        try:
            _ensure_service_cuda_device(svc)
            if static_mode:
                svc.preload_static_face(
                    face_path=AVATAR_IMAGE,
                    fps=AVATAR_FPS,
                    pads=FACE_PADS,
                )
            else:
                svc.preload_video_cache(
                    face_path=AVATAR_IMAGE,
                    fps=AVATAR_FPS,
                    pads=FACE_PADS,
                )
        except Exception as preload_error:
            suffix = f" #{index}" if len(service_pool) > 1 else ""
            print(f"âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒ Ð°Ð²Ð°Ñ‚Ð°Ñ€ Ð´Ð»Ñ ÑÐµÑ€Ð²Ð¸ÑÐ°{suffix}: {preload_error}")

    start_capture = time.perf_counter()
    if len(service_pool) == 1:
        frames, raw_stats = _capture_frames_single(
            primary_service,
            static_mode=static_mode,
            audio_waveform=audio_waveform,
            audio_sample_rate=audio_sample_rate,
            batch_size=batch_size,
        )
        active_chunks = 1
    else:
        frames, raw_stats, active_chunks = _capture_frames_parallel(
            service_pool,
            static_mode=static_mode,
            audio_waveform=audio_waveform,
            audio_sample_rate=audio_sample_rate,
            batch_size=batch_size,
            chunks=len(service_pool),
            fps=AVATAR_FPS,
        )
    capture_time = time.perf_counter() - start_capture

    total_frames = len(frames)
    if total_frames == 0:
        raise RuntimeError("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ°Ð´Ñ€Ñ‹ Ð´Ð»Ñ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ")

    requested_segments = segments if segments > 0 else None
    target_segments = segments if segments > 0 else 16
    safe_segments = max(1, min(total_frames, target_segments))

    if requested_segments is None:
        print(
            f"ðŸŽ¯ ÐÐ²Ñ‚Ð¾Ñ€Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ: Ð´Ð»Ð¸Ð½Ð° Ð°ÑƒÐ´Ð¸Ð¾ {audio_duration_seconds:.2f}s, ÐºÐ°Ð´Ñ€Ð¾Ð² {total_frames}, ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ñ‹ {safe_segments}"
        )
    else:
        print(
            f"ðŸŽ¯ Ð Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾: Ð·Ð°Ð¿Ñ€Ð¾Ñ {requested_segments}, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¾ {safe_segments} ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² (ÐºÐ°Ð´Ñ€Ð¾Ð² {total_frames})"
        )

    codec_name, codec_opts = primary_service._select_ffmpeg_codec()  # type: ignore[attr-defined]

    effective_fps = _resolve_effective_fps(raw_stats, total_frames, audio_duration_seconds)
    encoded_duration = (total_frames / effective_fps) if effective_fps > 0 else 0.0
    sync_delta = abs(encoded_duration - audio_duration_seconds)
    if sync_delta > 0.05:  # ~3 frames @ 60fps, warns about noticeable drift
        print(
            f"âš ï¸ ÐŸÑ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ðµ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸: Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ {encoded_duration:.3f}s Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² Ð°ÑƒÐ´Ð¸Ð¾ {audio_duration_seconds:.3f}s"
        )
    print(f"ðŸŽšï¸ Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð° ÐºÐ°Ð´Ñ€Ð¾Ð² Ð´Ð»Ñ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ: {effective_fps:.3f} fps")

    start_encode = time.perf_counter()
    segment_results, total_encode_time, segment_paths = _encode_segments(
        frames,
        effective_fps,
        codec_name,
        codec_opts,
        segments_dir,
        safe_segments,
    )

    merged_path = temp_root / "merged_no_audio.mp4"
    start_merge = time.perf_counter()
    merge_rc, merge_stderr = _concat_segments(segment_paths, merged_path)
    merge_time = time.perf_counter() - start_merge
    if merge_rc != 0:
        raise RuntimeError(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÐ»ÐµÐ¹ÐºÐ¸ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²: {merge_stderr}")

    final_output = Path(OUTPUT_DIR) / f"{job_id}.mp4"
    start_mux = time.perf_counter()
    mux_rc, mux_stderr = _attach_audio(merged_path, audio_path, final_output)
    audio_mux_time = time.perf_counter() - start_mux
    if mux_rc != 0:
        raise RuntimeError(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð°ÑƒÐ´Ð¸Ð¾: {mux_stderr}")

    normalized_stats = {k: _normalize_stat(v) for k, v in (raw_stats or {}).items()}
    normalized_stats["effective_fps"] = float(effective_fps)
    inference_time_wall = float(
        normalized_stats.get("inference_time_max")
        or normalized_stats.get("inference_time")
        or normalized_stats.get("process_time")
        or 0.0
    )

    timings = SegmentTiming(
        tts_time=tts_time,
        audio_convert_time=audio_convert_time,
        capture_time=capture_time,
        inference_time=inference_time_wall,
        encode_time=total_encode_time,
        merge_time=merge_time,
        audio_mux_time=audio_mux_time,
        total_time=time.perf_counter() - start_total,
    )

    result = SegmentJobResult(
        job_id=job_id,
        video_path=final_output,
        temp_dir=temp_root,
        segments=safe_segments,
        requested_segments=requested_segments,
        total_frames=total_frames,
        resolution=frames[0].shape[:2],
        timings=timings,
        segment_results=segment_results,
        inference_stats=normalized_stats,
        capture_workers=len(service_pool),
        capture_chunks=active_chunks if len(service_pool) > 1 else 1,
    )
    result.dump_metadata()
    return result


def load_segment_metadata(job_id: str) -> Optional[Dict[str, object]]:
    meta_path = Path(TEMP_DIR) / "segments" / job_id / SEGMENT_META_FILENAME
    if not meta_path.exists():
        return None
    with open(meta_path, "r", encoding="utf-8") as handle:
        return json.load(handle)


def _avatar_supports_video() -> bool:
    return Path(AVATAR_IMAGE).suffix.lower() in VIDEO_EXTENSIONS


def _save_waveform(path: Path, waveform, sample_rate: int) -> None:
    tensor = waveform.detach().cpu()
    save_kwargs = dict(
        filepath=str(path),
        src=tensor,
        sample_rate=sample_rate,
        channels_first=True,
        format="wav",
    )
    try:
        torchaudio.save(**save_kwargs, backend="sox_io")  # type: ignore[arg-type]
        return
    except TypeError:
        pass
    except RuntimeError as err:
        if "TorchCodec" not in str(err):
            raise

    # Fallback: manual WAV writer (PCM16)
    audio_np = tensor.numpy()
    if audio_np.ndim == 1:
        audio_np = audio_np[None, :]
    clipped = np.clip(audio_np, -1.0, 1.0)
    pcm16 = (clipped * 32767.0).round().astype(np.int16)
    interleaved = pcm16.T.reshape(-1)
    with wave.open(str(path), "wb") as wav_file:
        wav_file.setnchannels(pcm16.shape[0])
        wav_file.setsampwidth(2)
        wav_file.setframerate(sample_rate)
        wav_file.writeframes(interleaved.tobytes())


def _normalize_stat(value):
    if isinstance(value, numbers.Number):
        return float(value)
    try:
        return float(value)
    except (TypeError, ValueError):
        return value


def _resolve_effective_fps(
    raw_stats: Optional[Dict[str, float]],
    total_frames: int,
    audio_duration_seconds: float,
) -> float:
    if raw_stats:
        for key in ("fps", "video_fps"):
            candidate = raw_stats.get(key)
            normalized = _normalize_stat(candidate)
            if isinstance(normalized, numbers.Number) and normalized > 0:
                return float(normalized)

    if audio_duration_seconds > 0:
        derived = total_frames / audio_duration_seconds
        if derived > 0:
            return float(derived)

    return float(AVATAR_FPS)


def _partition_frames(total_frames: int, segments_count: int) -> List[Tuple[int, int]]:
    segments_count = max(1, segments_count)
    base = total_frames // segments_count
    remainder = total_frames % segments_count
    ranges: List[Tuple[int, int]] = []
    cursor = 0
    for index in range(segments_count):
        extra = 1 if index < remainder else 0
        start = cursor
        end = min(total_frames, cursor + base + extra)
        ranges.append((start, end))
        cursor = end
    return ranges


def _partition_samples(total_samples: int, parts: int) -> List[Tuple[int, int]]:
    parts = max(1, parts)
    base = total_samples // parts
    remainder = total_samples % parts
    ranges: List[Tuple[int, int]] = []
    cursor = 0
    for index in range(parts):
        extra = 1 if index < remainder else 0
        start = cursor
        end = min(total_samples, cursor + base + extra)
        ranges.append((start, end))
        cursor = end
    return ranges


def _build_ffmpeg_command(
    codec: str,
    codec_opts: List[str],
    width: int,
    height: int,
    fps: float,
    output_path: Path,
) -> List[str]:
    dimension = f"{width}x{height}"
    blocksize = width * height * 3
    vf_filters: List[str] = []
    if width % 2 != 0 or height % 2 != 0:
        vf_filters.append("scale=w=trunc(iw/2)*2:h=trunc(ih/2)*2:flags=fast_bilinear")
    command: List[str] = [
        "ffmpeg",
        "-y",
        "-f",
        "rawvideo",
        "-pix_fmt",
        "bgr24",
        "-s",
        dimension,
        "-r",
        str(fps),
        "-blocksize",
        str(blocksize),
        "-i",
        "pipe:0",
        "-c:v",
        codec,
    ]
    command += codec_opts
    if vf_filters:
        command += ["-vf", ",".join(vf_filters)]
    command += [
        "-pix_fmt",
        "yuv420p",
        "-an",
        str(output_path),
    ]
    return command


def _write_segment(proc: subprocess.Popen, frames: List[np.ndarray]) -> float:
    if proc.stdin is None:
        raise RuntimeError("FFmpeg stdin Ð½Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")
    start = time.perf_counter()
    for frame in frames:
        array = np.asarray(frame, dtype=np.uint8, order="C")
        if array.ndim != 3:
            raise ValueError("ÐÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ ÐºÐ°Ð´Ñ€Ð° Ð´Ð»Ñ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸")
        proc.stdin.write(array.tobytes())
    proc.stdin.close()
    return time.perf_counter() - start


def _encode_segments(
    frames: List[np.ndarray],
    fps: float,
    codec: str,
    codec_opts: List[str],
    output_dir: Path,
    segments_count: int,
) -> Tuple[List[SegmentEncodingResult], float, List[Path]]:
    total_frames = len(frames)
    if total_frames == 0:
        raise RuntimeError("ÐÐµÑ‚ ÐºÐ°Ð´Ñ€Ð¾Ð² Ð´Ð»Ñ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ")

    height, width = frames[0].shape[:2]
    frame_ranges = _partition_frames(total_frames, segments_count)

    results: List[SegmentEncodingResult] = []
    segment_paths: List[Path] = []
    threads: List[threading.Thread] = []
    results_lock = threading.Lock()
    start_global = time.perf_counter()

    for index, (start_frame, end_frame) in enumerate(frame_ranges):
        if start_frame >= end_frame:
            continue
        segment_frames = frames[start_frame:end_frame]
        output_path = output_dir / f"segment_{index:02d}.mp4"
        cmd = _build_ffmpeg_command(codec, codec_opts, width, height, fps, output_path)
        proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stderr=subprocess.PIPE)

        def _worker(idx: int, base: int, chunk: List[np.ndarray], popen: subprocess.Popen) -> None:
            write_time = _write_segment(popen, chunk)
            wait_start = time.perf_counter()
            return_code = popen.wait()
            ffmpeg_time = time.perf_counter() - wait_start
            stderr_data = popen.stderr.read().decode("utf-8", errors="ignore") if popen.stderr else ""
            result = SegmentEncodingResult(
                index=idx,
                frame_start=base,
                frame_end=base + len(chunk) - 1,
                frame_count=len(chunk),
                write_time=write_time,
                ffmpeg_time=ffmpeg_time,
                return_code=return_code,
                stderr=stderr_data,
            )
            with results_lock:
                results.append(result)

        thread = threading.Thread(
            target=_worker,
            args=(index, start_frame, segment_frames, proc),
            daemon=True,
        )
        segment_paths.append(output_path)
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    total_encode_time = time.perf_counter() - start_global
    results.sort(key=lambda item: item.index)
    return results, total_encode_time, segment_paths


def _concat_segments(segment_paths: List[Path], output_path: Path) -> Tuple[int, str]:
    if not segment_paths:
        raise RuntimeError("ÐÐµÑ‚ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ ÑÐºÐ»ÐµÐ¹ÐºÐ¸")

    concat_list = output_path.parent / "segments.txt"
    with open(concat_list, "w", encoding="utf-8") as handle:
        for path in segment_paths:
            resolved = path.resolve()
            handle.write(f"file '{resolved.as_posix()}'\n")

    command = [
        "ffmpeg",
        "-y",
        "-loglevel",
        "error",
        "-f",
        "concat",
        "-safe",
        "0",
        "-i",
        str(concat_list),
        "-c",
        "copy",
        str(output_path),
    ]
    proc = subprocess.run(command, capture_output=True, text=True)
    try:
        concat_list.unlink()
    except OSError:
        pass
    return proc.returncode, proc.stderr


def _attach_audio(video_path: Path, audio_path: Path, output_path: Path) -> Tuple[int, str]:
    command = [
        "ffmpeg",
        "-y",
        "-loglevel",
        "error",
        "-i",
        str(video_path),
        "-i",
        str(audio_path),
        "-c:v",
        "copy",
        "-c:a",
        "aac",
        "-ar",
        "16000",
        "-b:a",
        "128k",
        "-movflags",
        "+faststart",
        "-shortest",
        str(output_path),
    ]
    proc = subprocess.run(command, capture_output=True, text=True)
    return proc.returncode, proc.stderr


def _capture_frames_single(
    service,
    static_mode: bool,
    audio_waveform,
    audio_sample_rate: int,
    batch_size: int,
) -> Tuple[List[np.ndarray], Dict[str, float]]:
    _ensure_service_cuda_device(service)

    frames: List[np.ndarray] = []

    def _sink(frame: np.ndarray) -> None:
        frames.append(frame.copy())

    stats = service.process(
        face_path=AVATAR_IMAGE,
        audio_path="",
        output_path=None,
        static=static_mode,
        fps=AVATAR_FPS,
        pads=FACE_PADS,
        audio_waveform=audio_waveform,
        audio_sample_rate=audio_sample_rate,
        frame_sink=_sink,
        batch_size_override=batch_size,
    )
    return frames, stats or {}


def _capture_frames_parallel(
    services: List,
    static_mode: bool,
    audio_waveform,
    audio_sample_rate: int,
    batch_size: int,
    chunks: int,
    fps: float,
) -> Tuple[List[np.ndarray], Dict[str, float], int]:
    total_samples = int(audio_waveform.shape[1])
    sample_ranges = _partition_samples(total_samples, chunks)

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ†ÐµÐ»ÐµÐ²Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ°Ð´Ñ€Ð¾Ð² Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‡Ð°Ð½ÐºÐ°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑƒÐ±Ñ€Ð°Ñ‚ÑŒ
    # Ð½Ð°ÐºÐ¾Ð¿Ð¸Ð²ÑˆÐ¸Ð¹ÑÑ Ð´Ñ€ÐµÐ¹Ñ„ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸.
    expected_total_frames = int(round((total_samples / audio_sample_rate) * fps))
    expected_counts: List[int] = []
    accumulated_expected = 0.0
    assigned_so_far = 0
    for start_sample, end_sample in sample_ranges:
        if start_sample >= end_sample:
            expected_counts.append(0)
            continue
        duration = (end_sample - start_sample) / audio_sample_rate
        accumulated_expected += duration * fps
        target_frames = int(round(accumulated_expected)) - assigned_so_far
        if target_frames < 0:
            target_frames = 0
        expected_counts.append(target_frames)
        assigned_so_far += target_frames

    if assigned_so_far < expected_total_frames:
        deficit = expected_total_frames - assigned_so_far
        for index in range(len(expected_counts) - 1, -1, -1):
            if sample_ranges[index][1] > sample_ranges[index][0]:
                expected_counts[index] += deficit
                assigned_so_far += deficit
                break

    frames_per_chunk: List[Optional[List[np.ndarray]]] = [None] * len(sample_ranges)
    stats_per_chunk: List[Optional[Dict[str, float]]] = [None] * len(sample_ranges)
    errors: List[Exception] = []
    errors_lock = threading.Lock()
    threads: List[threading.Thread] = []

    active_chunks = 0

    for index, (start_sample, end_sample) in enumerate(sample_ranges):
        if start_sample >= end_sample:
            continue
        active_chunks += 1
        service = services[index % len(services)]
        waveform_slice = audio_waveform[:, start_sample:end_sample].contiguous()

        def _worker(idx: int, svc, chunk_waveform):
            try:
                chunk_frames, chunk_stats = _capture_frames_single(
                    svc,
                    static_mode=static_mode,
                    audio_waveform=chunk_waveform,
                    audio_sample_rate=audio_sample_rate,
                    batch_size=batch_size,
                )
                frames_per_chunk[idx] = chunk_frames
                stats_per_chunk[idx] = chunk_stats
            except Exception as worker_error:  # pragma: no cover - runtime safeguard
                with errors_lock:
                    errors.append(worker_error)

        thread = threading.Thread(
            target=_worker,
            args=(index, service, waveform_slice),
            daemon=True,
        )
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    if errors:
        raise errors[0]

    ordered_frames: List[np.ndarray] = []
    collected_stats: List[Dict[str, float]] = []
    dropped_total = 0

    for index, chunk in enumerate(frames_per_chunk):
        if not chunk:
            continue
        target = expected_counts[index] if index < len(expected_counts) else len(chunk)
        if target <= 0:
            target = 0
            trimmed_chunk: List[np.ndarray] = []
        elif len(chunk) > target:
            dropped_total += len(chunk) - target
            trimmed_chunk = chunk[:target]
        else:
            trimmed_chunk = chunk
            target = len(chunk)
        ordered_frames.extend(trimmed_chunk)
        stats_entry = stats_per_chunk[index]
        actual_count = len(trimmed_chunk)
        if stats_entry is not None:
            stats_entry['num_frames'] = actual_count
            if len(chunk) > actual_count:
                stats_entry['dropped_frames'] = len(chunk) - actual_count
        if stats_entry:
            collected_stats.append(stats_entry)

    if dropped_total > 0:
        print(f"âš™ï¸ ÐšÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸Ñ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸: ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {dropped_total} Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÐºÐ°Ð´Ñ€Ð¾Ð² Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸")

    merged_stats = _merge_stats(collected_stats) if collected_stats else {}
    return ordered_frames, merged_stats, active_chunks


def _merge_stats(stats_list: List[Dict[str, float]]) -> Dict[str, float]:
    numeric_totals: Dict[str, float] = {}
    numeric_max: Dict[str, float] = {}
    last_seen: Dict[str, float] = {}

    for stats in stats_list:
        for key, value in stats.items():
            normalized = _normalize_stat(value)
            if isinstance(normalized, numbers.Number):
                numeric_totals[key] = numeric_totals.get(key, 0.0) + float(normalized)
                numeric_max[key] = max(numeric_max.get(key, float(normalized)), float(normalized))
            else:
                last_seen[key] = normalized

    merged: Dict[str, float] = {key: float(val) for key, val in numeric_totals.items()}
    for key, val in last_seen.items():
        merged[key] = val
    for key, val in numeric_max.items():
        merged[f"{key}_max"] = val
    return merged
