"""Primary avatar generation endpoint."""
from __future__ import annotations

import os
import time
from datetime import datetime

from flask import jsonify, request, send_file

from ... import state
from ...config import AVATAR_FPS, AVATAR_IMAGE, OUTPUT_DIR
from ...services import convert_to_wav, generate_tts, estimate_optimal_chunks
from . import api_bp, register_route
from .helpers import (
    avatar_supports_dynamic,
    coerce_optional_bool,
    encode_video_with_audio,
    generate_frames_parallel,
    generate_frames_single,
    DEFAULT_BATCH_SIZE,
)


@api_bp.route("/api/generate", methods=["POST"])
def generate_avatar_speech():
    try:
        data = request.get_json()
        if not data or "text" not in data:
            return jsonify({"error": "–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–µ \"text\""}), 400

        text = data["text"].strip()
        language = data.get("language", "ru")

        if not text:
            return jsonify({"error": "–¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"}), 400
        if language not in ["ru", "kk", "en"]:
            return jsonify({"error": "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —è–∑—ã–∫"}), 400

        supports_dynamic = avatar_supports_dynamic()
        static_mode = state.avatar_static_mode
        if "static_mode" in data:
            try:
                requested_static = coerce_optional_bool(data.get("static_mode"))
            except ValueError as bool_err:
                return jsonify({"error": str(bool_err)}), 400

            if requested_static is False and not supports_dynamic:
                print(
                    "‚ö†Ô∏è –ó–∞–ø—Ä–æ—à–µ–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º, –Ω–æ —Ç–µ–∫—É—â–∏–π –∞–≤–∞—Ç–∞—Ä –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤–∏–¥–µ–æ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ç–∏—á–Ω—ã–π —Ä–µ–∂–∏–º."
                )
                static_mode = True
            else:
                static_mode = requested_static

        print("\n" + "=" * 60)
        print("üé¨ –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        print("=" * 60)
        print(f"–¢–µ–∫—Å—Ç: {text}")
        print(f"–Ø–∑—ã–∫: {language}")
        print(f"–†–µ–∂–∏–º –∞–≤–∞—Ç–∞—Ä–∞: {'—Å—Ç–∞—Ç–∏—á–Ω—ã–π' if static_mode else '–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π'}")

        start_total = time.time()

        start = time.time()
        audio_data = generate_tts(text, language)
        tts_time = time.time() - start

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        start = time.time()
        audio_waveform, audio_sample_rate = convert_to_wav(audio_data, None)
        convert_time = time.time() - start

        output_path = os.path.join(OUTPUT_DIR, f"avatar_{timestamp}.mp4")

        gan_services = [svc for svc in state.get_all_gan_services() if svc]
        service_pool = list(gan_services)
        if not service_pool and state.lipsync_service_nogan is not None:
            service_pool = [state.lipsync_service_nogan]

        if not service_pool:
            return jsonify({"error": "–ú–æ–¥–µ–ª—å lipsync –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}), 503

        primary_service = service_pool[0]

        audio_duration_seconds = audio_waveform.shape[1] / float(audio_sample_rate)
        base_segments = estimate_optimal_chunks(audio_duration_seconds, max(1, len(service_pool)))
        desired_segments = max(len(service_pool), base_segments) if base_segments > 1 else 1
        use_parallel = len(service_pool) > 1 and desired_segments > 1

        if use_parallel:
            device_labels = ", ".join(str(getattr(svc, "device", "cuda")) for svc in service_pool)
            print(f"üß† –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ {len(service_pool)} GPU: {device_labels}")
        else:
            model_label = "GAN" if primary_service is state.lipsync_service_gan else "NoGAN"
            print(f"üé≠ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è lip-sync ({model_label}, –µ–¥–∏–Ω–∏—á–Ω—ã–π –ø–æ—Ç–æ–∫)...")

        prepare_start = time.time()

        for index, svc in enumerate(service_pool, start=1):
            try:
                if static_mode:
                    svc.preload_static_face(
                        face_path=AVATAR_IMAGE,
                        fps=AVATAR_FPS,
                        pads=(0, 50, 0, 0),
                    )
                else:
                    svc.preload_video_cache(
                        face_path=AVATAR_IMAGE,
                        fps=AVATAR_FPS,
                        pads=(0, 50, 0, 0),
                    )
            except Exception as preload_error:
                suffix = f" #{index}" if len(service_pool) > 1 else ""
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∞–≤–∞—Ç–∞—Ä{suffix}: {preload_error}")

        prepare_time = time.time() - prepare_start

        frames_start = time.time()

        if use_parallel:
            frames, stats, active_chunks = generate_frames_parallel(
                service_pool=service_pool,
                static_mode=static_mode,
                audio_waveform=audio_waveform,
                audio_sample_rate=audio_sample_rate,
                desired_chunks=desired_segments,
                batch_size=DEFAULT_BATCH_SIZE,
            )
            print(
                f"‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(frames)} –∫–∞–¥—Ä–æ–≤, {active_chunks} —á–∞–Ω–∫–æ–≤ (–∑–∞–ø—Ä–æ—à–µ–Ω–æ {desired_segments})"
            )
        else:
            frames, stats = generate_frames_single(
                service=primary_service,
                static_mode=static_mode,
                audio_waveform=audio_waveform,
                audio_sample_rate=audio_sample_rate,
                batch_size=DEFAULT_BATCH_SIZE,
            )

        frames_time = time.time() - frames_start

        encode_start = time.time()
        encode_video_with_audio(
            frames=frames,
            output_path=output_path,
            audio_waveform=audio_waveform,
            audio_sample_rate=audio_sample_rate,
            fps=AVATAR_FPS,
            codec_service=primary_service,
            segments=desired_segments,
        )

        encode_time = time.time() - encode_start

        lipsync_time = prepare_time + frames_time
        pipeline_time = lipsync_time + encode_time
        total_time = time.time() - start_total

        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   TTS –≥–µ–Ω–µ—Ä–∞—Ü–∏—è:    {tts_time:.2f}s")
        print(f"   –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è:      {convert_time:.2f}s")
        print(f"   Lip-sync (prep):  {prepare_time:.2f}s")
        print(f"   Lip-sync (frames): {frames_time:.2f}s")
        print(f"   Lip-sync —Å—É–º–º–∞:   {lipsync_time:.2f}s")
        print(f"   –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ:      {encode_time:.2f}s")
        print(f"   –ü–∞–π–ø–ª–∞–π–Ω (–≤–∏–¥–µ–æ): {pipeline_time:.2f}s")
        if use_parallel:
            print(f"   –ê–∫—Ç–∏–≤–Ω—ã–µ GPU:     {len(service_pool)}")
            print(f"   –ß–∞–Ω–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:  {active_chunks}")
        else:
            print(f"     - –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ:   {stats.get('load_video_time', 0):.2f}s")
            print(f"     - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ:  {stats.get('process_audio_time', 0):.2f}s")
            print(f"     - –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü–∞:    {stats.get('face_detection_time', 0):.2f}s")
            print(f"     - –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏:  {stats.get('inference_time', 0):.2f}s")
        print(f"   –†–µ–∂–∏–º –∞–≤–∞—Ç–∞—Ä–∞:    {'—Å—Ç–∞—Ç–∏—á–Ω—ã–π' if static_mode else '–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π'}")
        print("   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        print(f"   –ò–¢–û–ì–û:            {total_time:.2f}s")
        print(f"\n‚úÖ –í–∏–¥–µ–æ –≥–æ—Ç–æ–≤–æ: {output_path}")
        print("=" * 60 + "\n")

        return send_file(
            output_path,
            mimetype="video/mp4",
            as_attachment=True,
            download_name="avatar_speech.mp4",
        )

    except Exception as exc:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {exc}")
        import traceback

        traceback.print_exc()
        return jsonify({"error": str(exc)}), 500


register_route("/r/api/generate", generate_avatar_speech, methods=["POST"])
